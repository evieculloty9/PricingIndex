name: Price-IQ / HCPI — Hourly

on:
  schedule:
    - cron: "5 * * * *" # run at minute 5 every hour (UTC)
  workflow_dispatch: {}   # allow manual run
  push:
    paths:
      - "master_runner.py"
      - "index_calculator.py"
      - "scrapers_all_providers.py"
      - "database_integration.py"
      - "roi_calculator.py"
      - "requirements.txt"
      - "docs/**"
      - ".github/workflows/index_hourly.yml"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    env:
      TZ: UTC
      PYTHONUNBUFFERED: "1"
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: pip-${{ runner.os }}-${{ hashFiles('requirements.txt') }}
          restore-keys: pip-${{ runner.os }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # If any scrapers use Playwright, uncomment the two lines below:
          # pip install playwright
          # python -m playwright install --with-deps chromium

      - name: Run pipeline (scrape → index → persist)
        env:
          HCPI_DATABASE_URL: ${{ secrets.HCPI_DATABASE_URL }}  # set this in repo Settings → Secrets and variables → Actions
        run: |
          set -euo pipefail
          python master_runner.py --cron --db "${HCPI_DATABASE_URL:-sqlite:///hcpi.db}" --verbose

      - name: Prepare static site (copy dashboard + data)
        run: |
          set -euo pipefail

          # public/ is the root we’ll deploy
          mkdir -p public

          # 1) Dashboard HTML (you put yours in docs/index.html)
          if [ -f docs/index.html ]; then
            cp -f docs/index.html public/index.html
          elif [ -f index.html ]; then
            cp -f index.html public/index.html
          else
            echo "No index.html found (docs/ or repo root)."
          fi

          # 2) Derived CSVs (the dashboard reads these)
          mkdir -p public/data/derived
          # copy anything you produce; ignore if missing
          for f in provider_scores_latest.csv roi_comparison.csv compute_calculator_snapshot.csv forecast.csv; do
            if [ -f "data/derived/$f" ]; then
              cp -f "data/derived/$f" "public/data/derived/$f"
            fi
          done

          # 3) HCPI JSONs (stable names used by the dashboard)
          mkdir -p public/hcpi
          # If master_runner wrote timestamped files, pick the newest and copy to stable names
          latest_full=$(ls -1t **/hcpi_*_full.json 2>/dev/null | head -n1 || true)
          latest_sum=$(ls -1t **/hcpi_*_summary.json 2>/dev/null | head -n1 || true)
          if [ -n "${latest_full}" ]; then
            cp -f "${latest_full}" public/hcpi/hcpi_latest_full.json
          elif [ -f "hcpi/hcpi_latest_full.json" ]; then
            cp -f "hcpi/hcpi_latest_full.json" public/hcpi/hcpi_latest_full.json
          fi
          if [ -n "${latest_sum}" ]; then
            cp -f "${latest_sum}" public/hcpi/hcpi_latest_summary.json
          elif [ -f "hcpi/hcpi_latest_summary.json" ]; then
            cp -f "hcpi/hcpi_latest_summary.json" public/hcpi/hcpi_latest_summary.json
          fi

          # 4) Optional health check
          python - <<'PY'
import json, os, time, pathlib
p = pathlib.Path("public/health.json")
p.write_text(json.dumps({"ok": True, "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}))
print("Wrote", p)
PY

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

      - name: Deploy to GitHub Pages
        uses: actions/deploy-pages@v4
