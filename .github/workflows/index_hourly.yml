name: Forward Compute Index ‚Äî Hourly Build & Deploy

on:
  workflow_dispatch:
  schedule:
    - cron: "5 * * * *"   # every hour at minute 5

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: "forward-compute-index"
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.12"
  COMMIT_DATA: "true"

jobs:
  build-and-bundle:
    runs-on: ubuntu-latest
    timeout-minutes: 40

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: "pip"

      - name: Install dependencies
        run: |
          set -e
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium || true
          playwright install-deps || true

      - name: Run hourly scrape + index (master runner)
        env:
          HCPI_DATABASE_URL: ${{ secrets.HCPI_DATABASE_URL }}
          LAMBDA_API_KEY:    ${{ secrets.LAMBDA_API_KEY }}
          RUNPOD_API_KEY:    ${{ secrets.RUNPOD_API_KEY }}
        run: |
          set -e
          echo "Running master_runner.py for ALL providers‚Ä¶"
          python master_runner.py --cron --db "${HCPI_DATABASE_URL:-sqlite:///hcpi.db}" \
            || echo "master_runner reported issues (continuing)"
          echo "Pipeline execution complete."

      # NEW: backfill legacy provider CSVs into hcpi/hcpi_history.json (H100 + On-Demand)
      - name: Backfill legacy history into HCPI
        run: |
          set -e
          if [ -f scripts/backfill_legacy_to_hcpi.py ] && ls data/history/*_history.csv >/dev/null 2>&1; then
            echo "Found legacy history files. Running backfill‚Ä¶"
            python scripts/backfill_legacy_to_hcpi.py --src data/history --write
            echo "Backfill complete. Preview last 5 points:"
            tail -n 10 hcpi/hcpi_history.json || true
          else
            echo "No legacy backfill needed (script or *_history.csv not present)."
          fi

      # NEW: Apply smoothing to historical index data for stability
      - name: Smooth historical index data
        run: |
          set -e
          if [ -f scripts/smooth_historical_index.py ] && [ -f hcpi/hcpi_history.json ]; then
            echo "Applying 24-hour rolling average smoothing to hcpi/hcpi_history.json‚Ä¶"
            python scripts/smooth_historical_index.py --in-place --window 24
            echo "Smoothing complete. Index volatility reduced."
          else
            echo "Smoothing script or hcpi_history.json not present, skipping."
          fi

      - name: Verify generated data & history
        run: |
          echo "==== Latest CSV ===="
          if [ -f data/derived/provider_scores_latest.csv ]; then
            wc -l data/derived/provider_scores_latest.csv || true
            echo "First 3 rows:"
            head -3 data/derived/provider_scores_latest.csv || true
            echo "Last 3 rows:"
            tail -3 data/derived/provider_scores_latest.csv || true
          else
            echo " ERROR: Latest CSV not found!"
            exit 1
          fi
          
          echo ""
          echo "==== History CSV ===="
          if [ -f data/history/provider_scores_history.csv ]; then
            echo "Total lines in history:"
            wc -l data/history/provider_scores_history.csv || true
            echo "Last 5 entries:"
            tail -5 data/history/provider_scores_history.csv || true
          else
            echo " WARNING: History CSV not found (will be created)"
          fi
          
          echo ""
          echo "==== HCPI History JSON ===="
          if [ -f hcpi/hcpi_history.json ]; then
            echo "Number of historical data points:"
            grep -o '"timestamp"' hcpi/hcpi_history.json | wc -l || true
            echo "Last entry:"
            tail -10 hcpi/hcpi_history.json || true
          else
            echo "WARNING: HCPI history JSON not found (will be created)"
          fi

      - name: Commit generated data to repository
        if: env.COMMIT_DATA == 'true'
        uses: EndBug/add-and-commit@v9
        with:
          add: |
            data/derived/*.csv
            data/history/*.csv
            hcpi/*.json
            hcpi/*history*.csv
          message: "ci: hourly data update [skip ci]"
          author_name: github-actions
          author_email: actions@github.com

      - name: üèóÔ∏è Prepare static site (public/)
        run: |
          set -e
          mkdir -p public/data/derived public/data/history public/hcpi
          
          echo "==== Copying HTML ===="
          if [ -f docs/index.html ]; then
            cp -v docs/index.html public/index.html
          elif [ -f index.html ]; then
            cp -v index.html public/index.html
          else
            echo '<!doctype html><meta charset="utf-8"><title>Forward Compute Index</title><body style="font:14px system-ui;padding:24px;color:#e7edf3;background:#0f1216"><h1>Forward Compute Index</h1><p>No dashboard found. Add <code>docs/index.html</code> to the repo.</p></body>' > public/index.html
          fi
          
          echo "==== Copying data files ===="
          cp -v data/derived/*.csv public/data/derived/ 2>/dev/null || true
          cp -v data/history/*.csv  public/data/history/  2>/dev/null || true
          cp -v hcpi/*.json         public/hcpi/          2>/dev/null || true
          cp -v hcpi/*history*.csv  public/hcpi/          2>/dev/null || true

          echo "==== Verify copied files ===="
          echo "Latest CSV:"
          wc -l public/data/derived/provider_scores_latest.csv || true
          echo "History CSV:"
          wc -l public/data/history/provider_scores_history.csv 2>/dev/null || echo "No history CSV yet"
          echo "HCPI History JSON:"
          ls -lh public/hcpi/hcpi_history.json 2>/dev/null || echo "No HCPI history JSON yet"

          echo "==== public/ tree ===="
          find public -type f -print

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

  deploy:
    needs: build-and-bundle
    runs-on: ubuntu-latest
    timeout-minutes: 10
    environment:
      name: github-pages
    permissions:
      pages: write
      id-token: write
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4


